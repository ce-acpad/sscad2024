- 
  id: "first-day" 
  title: "Quarta-Feira"
  date: "23/10/2024"
  blocks:
    - start: "13:30"
      end: "14:30"
      activities:
        - id: "k2"
          type: "keynote"
          url: "#k2"
          room: "A ser definida"
          title: "O Supercomputador Santos Dumont no cenário nacional e internacional das pesquisas de HPC e IA"
          authors: "Carla Osthoff (Laboratório Nacional de Computação Científica)"
          image: "Carla.jpg"
          short_bio: "Possui graduação em Engenharia Elétrica pela PUC/RJ, mestrado e doutorado em Engenharia de Sistemas e Computação COPPE/UFRJ. Atua na área de processamento de alto desempenho desde 1985, inicialmente em projetos de desenvolvimento de hardware de multiprocessadores paralelos distribuídos. Atualmente, é pesquisadora na área de Computação de Alto Desempenho do Laboratório Nacional de Computação Científica (LNCC), professora do Programa Multidisciplinar de Pós-Graduação do LNCC, coordena o Centro Nacional de Processamento de Alto Desempenho do LNCC(CENAPAD/LNCC), é membro do corpo Técnico-Científico do Comitê Consultivo do Supercomputador Santos Dumont, coordena o Setor de Processamento de Alto Desempenho do LNCC e diversos projetos de colaboração na área de Computação de Alto Desempenho."
          abstract: "O supercomputador SDumont, hospedado no LNCC /MCTIC, na cidade de Petrópolis-RJ, financiado pelo MCTIC para atender a comunidade acadêmica brasileira, de forma gratuita, atende atualmente a cerca de 240 projetos de pesquisa de 20 áreas de conhecimento, coordenados por instituições de ensino e pesquisa distribuídas em 12 estados brasileiros além de colaborações com instituições de pesquisas internacionais. Nesta palestra serão apresentadas as características da nova arquitetura do supercomputador de cerca de 23 PetaFlops e do futuro Supercomputador para IA, como parte do Plano Nacional de Inteligencia Artificial (PBIA). Em seguida, serão apresentadas as pesquisas e as colaborações desenvolvidas pelo setor de Processamento de Alto Desempenho do LNCC."
-
  id: "second-day"
  title: "Quinta-Feira"
  date: "24/10/2024"
-
  id: "third-day"
  title: "Sexta-Feira"
  date: "25/10/2024"
  blocks:
    - start: "10:00"
      end: "11:00"
      activities:
        - id: "k1"
          type: "keynote"
          url: "#k1"
          room: "A ser definida"
          title: "The Price Performance of Performance Models"
          authors: "Felix Wolf (Technical University of Darmstadt)"
          image: "FelixWolf.jpg"
          short_bio: "Felix Wolf is a full professor at the Department of Computer Science of the Technical University of Darmstadt in Germany, where he leads the Laboratory for Parallel Programming. He works on methods, tools, and algorithms that support developing and deploying parallel software systems in various life-cycle stages. Wolf received his Ph.D. degree from RWTH Aachen University in 2003. After working more than two years as a postdoc at the Innovative Computing Laboratory of the University of Tennessee, he was appointed research group leader at Juelich Supercomputing Centre. Between 2009 and 2015, he was head of the Laboratory for Parallel Programming at the German Research School for Simulation Sciences in Aachen and a full professor at RWTH Aachen University. Wolf has made major contributions to several open-source performance tools for parallel programs, including Scalasca, Score-P, and Extra-P. Moreover, he has initiated the Virtual Institute – High Productivity Supercomputing, an international initiative of HPC programming-tool builders aimed at enhancing, integrating, and deploying their products. He has published over 150 refereed articles on parallel computing, several of which have received awards."
          abstract: "To understand the scaling behavior of HPC applications, developers often use performance models. A performance model is a formula that expresses a critical performance metric, such as runtime, as a function of one or more execution parameters, such as core count and input size. Performance models offer quick insights on a very high level of abstraction, including predictions of future behavior. Given the complexity of today’s applications, which often combine several sophisticated algorithms, creating performance models manually is extremely laborious. Empirical performance modeling, the process of learning such models from performance data, offers a convenient alternative but comes with its own set of challenges.  The two most prominent ones are noise and the cost of the experiments needed to generate the underlying data. In this talk, we will review the state of the art in empirical performance modeling and investigate how we can employ machine learning and other strategies to improve the quality and lower the cost of the resulting models."
